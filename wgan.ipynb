{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eac38fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s25090/miniconda3/envs/discovr/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import random as rand\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch import nn, Tensor\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional\n",
    "from torch.nn import functional as F\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from math import ceil\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.inception import InceptionScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62e4988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR(Dataset):\n",
    "    def __init__(self, path=\"/scratch/s25090/archive/cifar-10/train\", dataset:Optional[list]=None):\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.files = os.listdir(self.path) if dataset is None else dataset\n",
    "        self.T = T.Compose([\n",
    "           T.ToImage(), \n",
    "           T.ToDtype(torch.float32, scale=True),\n",
    "           T.Resize((32, 32)),\n",
    "           T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "\n",
    "    def __len__(self,):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file = self.files[idx]\n",
    "        img_path = os.path.join(self.path, file)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = self.T(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88b2f511",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, is_final):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channel, (out_channel+in_channel)//2, 3, 1, 1),\n",
    "            nn.BatchNorm2d((out_channel+in_channel)//2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d((in_channel+out_channel)//2, out_channel, 3, 1, 1)\n",
    "        ]\n",
    "        \n",
    "        if not is_final:\n",
    "            layers.append(nn.BatchNorm2d(out_channel))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        layers.append(nn.UpsamplingNearest2d(scale_factor=2))\n",
    "        self.layer = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class DisBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, (out_channel+in_channel)//2, 3, 1, 1),\n",
    "            nn.BatchNorm2d((out_channel+in_channel)//2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d((out_channel+in_channel)//2, out_channel, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channel),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class ResGenBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            \n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)\n",
    "        )\n",
    "        \n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_block(x) + self.shortcut(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7f95734",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=100):\n",
    "        super().__init__()\n",
    "        self.initial_linear = nn.Linear(z_dim, 1024 * 4 * 4)\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            GenBlock(1024, 512, is_final=False), \n",
    "            GenBlock(512, 256, is_final=False),  \n",
    "            GenBlock(256, 128,  is_final=False), \n",
    "            GenBlock(128,  64,  is_final=False),\n",
    "            GenBlock(64,  64,  is_final=True),\n",
    "        )\n",
    "        \n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.Conv2d(64, 3, kernel_size=3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        if len(z.shape) > 2:\n",
    "            z = z.view(z.size(0), -1)\n",
    "            \n",
    "        x = self.initial_linear(z)\n",
    "        x = x.view(-1, 1024, 4, 4)\n",
    "        x = self.net(x)\n",
    "        return self.final_layer(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            DisBlock(3, 32),   \n",
    "            DisBlock(32, 64),\n",
    "            DisBlock(64, 128),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 4 * 4, 1) \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "class ResNetGenerator(nn.Module):\n",
    "    def __init__(self, z_dim=100, base_channels=256):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(z_dim, 4 * 4 * base_channels)\n",
    "        self.base_channels = base_channels\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            ResGenBlock(base_channels, base_channels),    \n",
    "            ResGenBlock(base_channels, base_channels // 2), \n",
    "            ResGenBlock(base_channels // 2, base_channels // 4),\n",
    "        )\n",
    "        \n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.BatchNorm2d(base_channels // 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base_channels // 4, 3, 3, 1, 1), \n",
    "            nn.Tanh() \n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        if z.ndim > 2: z = z.view(z.size(0), -1)\n",
    "            \n",
    "        x = self.linear(z)\n",
    "        x = x.view(-1, self.base_channels, 4, 4)\n",
    "        x = self.blocks(x)\n",
    "        return self.final_layer(x)\n",
    "\n",
    "class WGANModel(nn.Module):\n",
    "    def __init__(self, z_dim=100, is_res=True):\n",
    "        super().__init__()\n",
    "        self.generator = Generator(z_dim) if not is_res else ResNetGenerator()\n",
    "        self.discriminator = Discriminator()\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "    def compute_discriminator_loss(self, real_imgs, z):\n",
    "        with torch.no_grad():\n",
    "            fake_imgs = self.generator(z).detach()\n",
    "\n",
    "        real_logits = self.discriminator(real_imgs)\n",
    "        fake_logits = self.discriminator(fake_imgs)\n",
    "\n",
    "        d_loss = -(torch.mean(real_logits) - torch.mean(fake_logits))\n",
    "        \n",
    "        return d_loss\n",
    "\n",
    "    def compute_generator_loss(self, z):\n",
    "        fake_imgs = self.generator(z)\n",
    "        \n",
    "        fake_logits = self.discriminator(fake_imgs)\n",
    "        \n",
    "        g_loss = -torch.mean(fake_logits)\n",
    "        \n",
    "        return g_loss, fake_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d338ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 5e-5        \n",
    "WEIGHT_CLIP = 0.01\n",
    "N_CRITIC = 5  \n",
    "DEVICE = 'cuda:3' if torch.cuda.is_available() else 'cpu'\n",
    "epochs = 300\n",
    "\n",
    "gan_model = WGANModel()\n",
    "gan_model = gan_model.to(DEVICE)\n",
    "\n",
    "opt_gen = torch.optim.RMSprop(gan_model.generator.parameters(), lr=LEARNING_RATE)\n",
    "opt_dis = torch.optim.RMSprop(gan_model.discriminator.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_dataset = CIFAR()\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "gen_loss_list = []\n",
    "dis_loss_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    gan_model.train()\n",
    "    tqdm_data = tqdm(train_loader, desc=f\"Epoch-{epoch+1}/{epochs}\")\n",
    "    \n",
    "    batch_gen_loss = 0\n",
    "    batch_dis_loss = 0\n",
    "    \n",
    "    for batch_idx, (real_img) in enumerate(tqdm_data):\n",
    "        real_img = real_img.to(DEVICE)\n",
    "        bs = real_img.size(0)\n",
    "\n",
    "        for param in gan_model.discriminator.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        z_dis = torch.randn(bs, 100).to(DEVICE)\n",
    "        opt_dis.zero_grad()\n",
    "        \n",
    "        dis_loss = gan_model.compute_discriminator_loss(real_img, z_dis)\n",
    "        dis_loss.backward()\n",
    "        opt_dis.step()\n",
    "\n",
    "        for p in gan_model.discriminator.parameters():\n",
    "            p.data.clamp_(-WEIGHT_CLIP, WEIGHT_CLIP)\n",
    "        \n",
    "        batch_dis_loss += dis_loss.item()\n",
    "\n",
    "        if batch_idx % N_CRITIC == 0:\n",
    "            for param in gan_model.discriminator.parameters():\n",
    "                param.requires_grad = False # Freeze D to save computation\n",
    "            \n",
    "            z = torch.randn(bs, 100).to(DEVICE)\n",
    "            opt_gen.zero_grad()\n",
    "            \n",
    "            gen_loss, fake_img = gan_model.compute_generator_loss(z)\n",
    "            gen_loss.backward()\n",
    "            opt_gen.step()\n",
    "            \n",
    "            batch_gen_loss += gen_loss.item()\n",
    "            current_gen_loss = gen_loss.item() # For tqdm\n",
    "        else:\n",
    "            current_gen_loss = batch_gen_loss / (batch_idx + 1) if batch_idx > 0 else 0\n",
    "\n",
    "        tqdm_data.set_postfix({\n",
    "            \"GenLoss\": current_gen_loss,\n",
    "            \"DisLoss\": dis_loss.item()\n",
    "        })\n",
    "\n",
    "    avg_gen_loss = batch_gen_loss / (len(train_loader) / N_CRITIC)\n",
    "    avg_dis_loss = batch_dis_loss / len(train_loader)\n",
    "    \n",
    "    gen_loss_list.append(avg_gen_loss)\n",
    "    dis_loss_list.append(avg_dis_loss)\n",
    "\n",
    "    print(f\"Generator Loss: {avg_gen_loss:.4f}\\nDiscriminator Loss: {avg_dis_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save(gan_model.state_dict(), f'/scratch/s25090/wgan_outputs/weights/Experiment1/gan_epoch_{epoch+1}.pth')\n",
    "        gan_model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_z = torch.randn(8, 100).to(DEVICE)\n",
    "            gan_image = gan_model.generator(test_z)\n",
    "            \n",
    "            comparison = torch.cat([real_img[:8], gan_image[:8]], dim=0)\n",
    "            grid = make_grid(comparison.cpu(), nrow=8, padding=2, normalize=True)\n",
    "            \n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.imshow(grid.permute(1, 2, 0))\n",
    "            plt.axis('off')\n",
    "            plt.title(f'Top: Original | Bottom: Generated Image (Epoch {epoch+1})')\n",
    "            plt.savefig(f\"/scratch/s25090/wgan_outputs/plots/Experiment1/Epoch-{epoch+1}.png\")\n",
    "            plt.close()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Generator vs Discriminator Loss (WGAN)\")\n",
    "plt.plot(gen_loss_list, label=\"Generator\")\n",
    "plt.plot(dis_loss_list, label=\"Discriminator\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Wasserstein Loss\")\n",
    "plt.savefig(f\"/scratch/s25090/wgan_outputs/plots/Experiment1_loss.png\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba99ab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(critic, real_samples, fake_samples, device):\n",
    "    \"\"\"\n",
    "    Calculates the gradient penalty loss for WGAN GP.\n",
    "    \"\"\"\n",
    "    alpha = torch.rand((real_samples.size(0), 1, 1, 1)).to(device)\n",
    "    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "    \n",
    "    d_interpolates = critic(interpolates)\n",
    "    fake = torch.ones(d_interpolates.shape).to(device)\n",
    "    \n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    \n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    \n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    \n",
    "    gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n",
    "    \n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe76e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4 \n",
    "BATCH_SIZE = 64\n",
    "CHANNELS_IMG = 3\n",
    "Z_DIM = 100\n",
    "NUM_EPOCHS = 100\n",
    "CRITIC_ITERATIONS = 5\n",
    "LAMBDA_GP = 10\n",
    "DEVICE = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "WEIGHT_PATH = \"/scratch/s25090/wgan_GP_outputs/weights/Experiment1\"\n",
    "PLOT_PATH = \"/scratch/s25090/wgan_GP_outputs/plots/Experiment1\"\n",
    "os.makedirs(WEIGHT_PATH, exist_ok=True)\n",
    "os.makedirs(PLOT_PATH, exist_ok=True)\n",
    "gan_model = WGANModel(z_dim=Z_DIM).to(DEVICE)\n",
    "gan_model.train()\n",
    "\n",
    "opt_gen = torch.optim.Adam(gan_model.generator.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
    "opt_critic = torch.optim.Adam(gan_model.discriminator.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
    "train_dataset = CIFAR()\n",
    "loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(\"Starting WGAN-GP Training...\")\n",
    "gen_loss_list = []\n",
    "dis_loss_list = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    loop = tqdm(loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    batch_gen_loss = 0\n",
    "    batch_dis_loss = 0\n",
    "    \n",
    "    for batch_idx, (real) in enumerate(loop):\n",
    "        real = real.to(DEVICE)\n",
    "        cur_batch_size = real.shape[0]\n",
    "\n",
    "        noise = torch.randn(cur_batch_size, Z_DIM).to(DEVICE)\n",
    "        fake = gan_model.generator(noise)\n",
    "        \n",
    "        critic_real = gan_model.discriminator(real).reshape(-1)\n",
    "        critic_fake = gan_model.discriminator(fake).reshape(-1)\n",
    "        \n",
    "        loss_critic_wasserstein = -(torch.mean(critic_real) - torch.mean(critic_fake))\n",
    "        \n",
    "        gp = compute_gradient_penalty(gan_model.discriminator, real, fake, DEVICE)\n",
    "        \n",
    "        loss_critic = loss_critic_wasserstein + (LAMBDA_GP * gp)\n",
    "        \n",
    "        opt_critic.zero_grad()\n",
    "        loss_critic.backward(retain_graph=True)\n",
    "        opt_critic.step()\n",
    "        batch_dis_loss += loss_critic.item()\n",
    "        batch_gen_loss += -torch.mean(critic_fake).item()\n",
    "        if batch_idx % CRITIC_ITERATIONS == 0:\n",
    "            gen_fake = gan_model.discriminator(fake).reshape(-1)\n",
    "            loss_gen = -torch.mean(gen_fake)\n",
    "            \n",
    "            opt_gen.zero_grad()\n",
    "            loss_gen.backward()\n",
    "            opt_gen.step()\n",
    "            \n",
    "            loop.set_postfix(\n",
    "                loss_critic=loss_critic.item(),\n",
    "                loss_gen=loss_gen.item(),\n",
    "                gp=gp.item()\n",
    "            )\n",
    "\n",
    "    avg_gen_loss = batch_gen_loss / (len(loader) / CRITIC_ITERATIONS)\n",
    "    avg_dis_loss = batch_dis_loss / len(loader)\n",
    "    \n",
    "    gen_loss_list.append(avg_gen_loss)\n",
    "    dis_loss_list.append(avg_dis_loss)\n",
    "\n",
    "    print(f\"Generator Loss: {avg_gen_loss:.4f}\\nDiscriminator Loss: {avg_dis_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save(gan_model.state_dict(), f\"{WEIGHT_PATH}/gan_epoch_{epoch+1}.pth\")\n",
    "        gan_model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_z = torch.randn(8, 100).to(DEVICE)\n",
    "            gan_image = gan_model.generator(test_z)\n",
    "            \n",
    "            comparison = torch.cat([real[:8], gan_image[:8]], dim=0)\n",
    "            grid = make_grid(comparison.cpu(), nrow=8, padding=2, normalize=True)\n",
    "            \n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.imshow(grid.permute(1, 2, 0))\n",
    "            plt.axis('off')\n",
    "            plt.title(f'Top: Original | Bottom: Generated Image (Epoch {epoch+1})')\n",
    "            plt.savefig(f\"{PLOT_PATH}/Epoch-{epoch+1}.png\")\n",
    "            plt.close()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Generator vs Discriminator Loss (WGAN)\")\n",
    "plt.plot(gen_loss_list, label=\"Generator\")\n",
    "plt.plot(dis_loss_list, label=\"Discriminator\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Wasserstein Loss\")\n",
    "plt.savefig(f\"{PLOT_PATH}/Experiment1_loss.png\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a097cf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "wgan_model = WGANModel()\n",
    "wgangp_model = WGANModel()\n",
    "wgan_model = wgan_model.to(DEVICE)\n",
    "wgangp_model = wgangp_model.to(DEVICE)\n",
    "WEIGHT_GP = torch.load(\"wgan_GP_outputs/gan_epoch_100.pth\")\n",
    "WEIGHT = torch.load(\"wgan_outputs/gan_epoch_300.pth\")\n",
    "wgangp_model.load_state_dict(WEIGHT_GP)\n",
    "wgan_model.load_state_dict(WEIGHT) \n",
    "test_loader = DataLoader(CIFAR(), batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8238cb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating WGAN...\n",
      "--- Computing Metrics (Samples: 2000) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Real Images:   4%|▍         | 16/391 [00:39<15:30,  2.48s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalizing calculations...\n",
      "FID: 65.8375 | IS: 3.8508\n",
      "Evaluating WGAN-GP...\n",
      "--- Computing Metrics (Samples: 2000) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Real Images:   4%|▍         | 16/391 [00:05<02:08,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalizing calculations...\n",
      "FID: 60.8983 | IS: 3.9543\n"
     ]
    }
   ],
   "source": [
    "def get_evaluation_metrics(generator, dataloader, device, num_imgs=10000):\n",
    "    \"\"\"\n",
    "    Calculates FID and IS for a GAN generator.\n",
    "    \n",
    "    Args:\n",
    "        generator: The GAN generator model.\n",
    "        dataloader: DataLoader for real images (needed for FID reference).\n",
    "        device: 'cuda' or 'cpu'.\n",
    "        num_imgs: Number of images to generate/use for calculation.\n",
    "                  (Standard for papers is 50k, but 10k is faster for debugging).\n",
    "    \n",
    "    Returns:\n",
    "        fid_score (float), is_score (float)\n",
    "    \"\"\"\n",
    "    fid = FrechetInceptionDistance(feature=2048, normalize=True).to(device)\n",
    "    inception = InceptionScore(normalize=True).to(device)\n",
    "    \n",
    "    generator.eval()\n",
    "    \n",
    "    print(f\"--- Computing Metrics (Samples: {num_imgs}) ---\")\n",
    "    \n",
    "    real_count = 0\n",
    "    for batch in tqdm(dataloader, desc=\"Processing Real Images\"):\n",
    "        batch = batch.to(device)\n",
    "        remaining = num_imgs - real_count\n",
    "        if remaining <= 0:\n",
    "            break\n",
    "            \n",
    "        if batch.shape[0] > remaining:\n",
    "            batch = batch[:remaining]\n",
    "        if batch.min() < 0:\n",
    "            batch = (batch + 1) / 2  # Now [0, 1]\n",
    "            \n",
    "        fid.update((batch * 255).to(torch.uint8), real=True)\n",
    "        real_count += batch.shape[0]\n",
    "\n",
    "    fake_count = 0\n",
    "    while fake_count < num_imgs:\n",
    "        batch_size = min(dataloader.batch_size, num_imgs - fake_count)\n",
    "        \n",
    "        z = torch.randn(batch_size, 100).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            fake_imgs = generator(z)\n",
    "        fake_imgs = (fake_imgs + 1) / 2\n",
    "        fake_uint8 = (fake_imgs * 255).to(torch.uint8)\n",
    "        \n",
    "        fid.update(fake_uint8, real=False)\n",
    "        inception.update(fake_uint8)\n",
    "        \n",
    "        fake_count += batch_size\n",
    "        \n",
    "    print(\"Finalizing calculations...\")\n",
    "    fid_score = fid.compute().item()\n",
    "    is_score_mean, is_score_std = inception.compute()\n",
    "    \n",
    "    return fid_score, is_score_mean.item()\n",
    "print(\"Evaluating WGAN...\")\n",
    "fid_wgan, is_score_wgan = get_evaluation_metrics(\n",
    "    wgan_model.generator, \n",
    "    test_loader, \n",
    "    DEVICE, \n",
    "    num_imgs=2000\n",
    ")\n",
    "print(f\"FID: {fid_wgan:.4f} | IS: {is_score_wgan:.4f}\")\n",
    "\n",
    "print(\"Evaluating WGAN-GP...\")\n",
    "fig_wgan_gp, is_score_wgan_gp = get_evaluation_metrics(\n",
    "    wgangp_model.generator, \n",
    "    test_loader, \n",
    "    DEVICE, \n",
    "    num_imgs=2000\n",
    ")\n",
    "print(f\"FID: {fig_wgan_gp:.4f} | IS: {is_score_wgan_gp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82eded4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "discovr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
