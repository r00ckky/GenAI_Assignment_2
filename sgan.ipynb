{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb170472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s25090/miniconda3/envs/discovr/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import random as rand\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import utils\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional\n",
    "from torch.nn import functional as F\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from math import ceil\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.inception import InceptionScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7945202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR(Dataset):\n",
    "    def __init__(self, path=\"/scratch/s25090/archive/cifar-10/train\", dataset:Optional[list]=None):\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.files = os.listdir(self.path) if dataset is None else dataset\n",
    "        self.T = T.Compose([\n",
    "           T.ToImage(), \n",
    "           T.ToDtype(torch.float32, scale=True),\n",
    "           T.Resize((32, 32)),\n",
    "           T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "\n",
    "    def __len__(self,):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file = self.files[idx]\n",
    "        img_path = os.path.join(self.path, file)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = self.T(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a3408a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNDiscriminator(nn.Module):\n",
    "    def __init__(self, channels_img=3, features_d=64):\n",
    "        super(SNDiscriminator, self).__init__()\n",
    "        \n",
    "        # Helper to create a spectral normalized block\n",
    "        def sn_block(in_channels, out_channels, kernel_size, stride, padding):\n",
    "            return nn.Sequential(\n",
    "                # WRAP THE CONV LAYER WITH SPECTRAL_NORM\n",
    "                utils.spectral_norm(\n",
    "                    nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "                ),\n",
    "                nn.LeakyReLU(0.2), \n",
    "                # Note: SN-GANs usually remove Batch Norm in the Discriminator entirely\n",
    "            )\n",
    "\n",
    "        self.disc = nn.Sequential(\n",
    "            # Input: N x 3 x 32 x 32\n",
    "            sn_block(channels_img, features_d, 3, 1, 1),\n",
    "            \n",
    "            # Downsampling blocks\n",
    "            sn_block(features_d, features_d * 2, 4, 2, 1),\n",
    "            sn_block(features_d * 2, features_d * 4, 4, 2, 1),\n",
    "            sn_block(features_d * 4, features_d * 8, 4, 2, 1),\n",
    "            \n",
    "            # Final output layer (Single scalar score)\n",
    "            # We also apply Spectral Norm to the final layer\n",
    "            utils.spectral_norm(nn.Conv2d(features_d * 8, 1, 4, 1, 0)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x).view(-1)\n",
    "\n",
    "class ResGenBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            \n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)\n",
    "        )\n",
    "        \n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_block(x) + self.shortcut(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e6b91a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetGenerator(nn.Module):\n",
    "    def __init__(self, z_dim=100, features_g=256, channels_img=3):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(z_dim, 4 * 4 * features_g)\n",
    "        self.features_g = features_g\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            ResGenBlock(features_g, features_g),\n",
    "            ResGenBlock(features_g, features_g),  \n",
    "            ResGenBlock(features_g, features_g),  \n",
    "            \n",
    "            nn.BatchNorm2d(features_g),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(features_g, channels_img, 3, 1, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.linear(z)\n",
    "        x = x.view(-1, self.features_g, 4, 4)\n",
    "        return self.net(x)\n",
    "\n",
    "class SNDiscriminator(nn.Module):\n",
    "    def __init__(self, channels_img=3, features_d=64):\n",
    "        super(SNDiscriminator, self).__init__()\n",
    "        \n",
    "        def sn_block(in_channels, out_channels, kernel_size, stride, padding):\n",
    "            return nn.Sequential(\n",
    "                utils.spectral_norm(\n",
    "                    nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "                ),\n",
    "                nn.LeakyReLU(0.2, inplace=True), \n",
    "            )\n",
    "\n",
    "        self.disc = nn.Sequential(\n",
    "            sn_block(channels_img, features_d, 3, 1, 1),        \n",
    "            sn_block(features_d, features_d * 2, 4, 2, 1),      \n",
    "            sn_block(features_d * 2, features_d * 4, 4, 2, 1),\n",
    "            sn_block(features_d * 4, features_d * 8, 4, 2, 1),\n",
    "            utils.spectral_norm(nn.Conv2d(features_d * 8, 1, 4, 1, 0)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x).view(-1)\n",
    "    \n",
    "class SGANModel(nn.Module):\n",
    "    def __init__(self, z_dim=100, channels_img=3, features_g=256, features_d=64):\n",
    "        super().__init__()\n",
    "        self.generator = ResNetGenerator(z_dim, features_g, channels_img)\n",
    "        self.discriminator = SNDiscriminator(channels_img, features_d)\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "    def compute_discriminator_loss(self, real_imgs, z):\n",
    "        with torch.no_grad():\n",
    "            fake_imgs = self.generator(z).detach()\n",
    "\n",
    "        real_logits = self.discriminator(real_imgs)\n",
    "        fake_logits = self.discriminator(fake_imgs)\n",
    "        \n",
    "        loss_real = torch.mean(F.relu(1.0 - real_logits))\n",
    "        loss_fake = torch.mean(F.relu(1.0 + fake_logits))\n",
    "        \n",
    "        d_loss = loss_real + loss_fake\n",
    "        return d_loss\n",
    "\n",
    "    def compute_generator_loss(self, z):\n",
    "        fake_imgs = self.generator(z)\n",
    "        fake_logits = self.discriminator(fake_imgs)\n",
    "        \n",
    "        g_loss = -torch.mean(fake_logits)\n",
    "        \n",
    "        return g_loss, fake_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7610c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2e-4\n",
    "BATCH_SIZE = 64\n",
    "CHANNELS_IMG = 3\n",
    "Z_DIM = 100\n",
    "NUM_EPOCHS = 200\n",
    "DISC_ITERATIONS = 5\n",
    "DEVICE = \"cuda:4\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "WEIGHT_PATH = \"/scratch/s25090/sngan_outputs/weights/Experiment1\"\n",
    "PLOT_PATH = \"/scratch/s25090/sngan_outputs/plots/Experiment1\"\n",
    "os.makedirs(WEIGHT_PATH, exist_ok=True)\n",
    "os.makedirs(PLOT_PATH, exist_ok=True)\n",
    "\n",
    "gan_model = SGANModel(z_dim=Z_DIM, channels_img=CHANNELS_IMG).to(DEVICE)\n",
    "gan_model.train()\n",
    "\n",
    "opt_gen = torch.optim.Adam(gan_model.generator.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
    "opt_dis = torch.optim.Adam(gan_model.discriminator.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
    "\n",
    "train_dataset = CIFAR()\n",
    "loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(\"Starting SN-GAN Training (Hinge Loss)...\")\n",
    "gen_loss_list = []\n",
    "dis_loss_list = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    loop = tqdm(loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    epoch_gen_loss = 0\n",
    "    epoch_dis_loss = 0\n",
    "    batches_processed = 0\n",
    "    \n",
    "    for batch_idx, (real_img) in enumerate(loop):\n",
    "        if isinstance(real_img, list) or isinstance(real_img, tuple):\n",
    "            real_img = real_img[0]\n",
    "            \n",
    "        real_img = real_img.to(DEVICE)\n",
    "        cur_batch_size = real_img.shape[0]\n",
    "        batches_processed += 1\n",
    "\n",
    "        opt_dis.zero_grad()\n",
    "        z = torch.randn(cur_batch_size, Z_DIM).to(DEVICE)\n",
    "        fake_img = gan_model.generator(z)\n",
    "        d_real = gan_model.discriminator(real_img).reshape(-1)\n",
    "        d_fake = gan_model.discriminator(fake_img.detach()).reshape(-1)\n",
    "        loss_d_real = torch.mean(F.relu(1.0 - d_real))\n",
    "        loss_d_fake = torch.mean(F.relu(1.0 + d_fake))\n",
    "        \n",
    "        loss_dis = loss_d_real + loss_d_fake\n",
    "        \n",
    "        loss_dis.backward()\n",
    "        opt_dis.step()\n",
    "        \n",
    "        epoch_dis_loss += loss_dis.item()\n",
    "\n",
    "        loss_gen_item = 0 \n",
    "        \n",
    "        if batch_idx % DISC_ITERATIONS == 0:\n",
    "            opt_gen.zero_grad()\n",
    "            \n",
    "            gen_fake_logits = gan_model.discriminator(fake_img).reshape(-1)\n",
    "            \n",
    "            loss_gen = -torch.mean(gen_fake_logits)\n",
    "            \n",
    "            loss_gen.backward()\n",
    "            opt_gen.step()\n",
    "            \n",
    "            loss_gen_item = loss_gen.item()\n",
    "            epoch_gen_loss += loss_gen_item\n",
    "            \n",
    "            loop.set_postfix(\n",
    "                d_loss=loss_dis.item(),\n",
    "                g_loss=loss_gen.item()\n",
    "            )\n",
    "\n",
    "    avg_dis_loss = epoch_dis_loss / len(loader)\n",
    "    avg_gen_loss = epoch_gen_loss / (len(loader) / DISC_ITERATIONS)\n",
    "    \n",
    "    gen_loss_list.append(avg_gen_loss)\n",
    "    dis_loss_list.append(avg_dis_loss)\n",
    "\n",
    "    print(f\"Generator Loss: {avg_gen_loss:.4f} | Discriminator Loss: {avg_dis_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save(gan_model.state_dict(), f\"{WEIGHT_PATH}/sngan_epoch_{epoch+1}.pth\")\n",
    "        \n",
    "        gan_model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_z = torch.randn(8, Z_DIM).to(DEVICE)\n",
    "            gan_images = gan_model.generator(test_z)\n",
    "            \n",
    "            comparison = torch.cat([real_img[:8], gan_images[:8]], dim=0)\n",
    "            grid = make_grid(comparison.cpu(), nrow=8, padding=2, normalize=True)\n",
    "            \n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.imshow(grid.permute(1, 2, 0))\n",
    "            plt.axis('off')\n",
    "            plt.title(f'Top: Original | Bottom: Generated Image (Epoch {epoch+1})')\n",
    "            plt.savefig(f\"{PLOT_PATH}/Epoch-{epoch+1}.png\")\n",
    "            plt.close()\n",
    "        gan_model.train()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"SN-GAN Hinge Loss\")\n",
    "plt.plot(gen_loss_list, label=\"Generator\")\n",
    "plt.plot(dis_loss_list, label=\"Discriminator\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Hinge Loss\")\n",
    "plt.savefig(f\"{PLOT_PATH}/Experiment1_loss.png\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f48e6e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:4\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SGANModel()\n",
    "WEIGHT = torch.load(\"sngan_outputs/sngan_epoch_200.pth\")\n",
    "model.load_state_dict(WEIGHT)\n",
    "model.to(DEVICE)\n",
    "train_loader = DataLoader(CIFAR(), batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18d26121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating WGAN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s25090/miniconda3/envs/discovr/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Computing Metrics (Samples: 2000) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Real Images:   4%|‚ñç         | 16/391 [00:44<17:27,  2.79s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalizing calculations...\n",
      "FID: 50.3581 | IS: 4.5761\n"
     ]
    }
   ],
   "source": [
    "def get_evaluation_metrics(generator, dataloader, device, num_imgs=10000):\n",
    "    \"\"\"\n",
    "    Calculates FID and IS for a GAN generator.\n",
    "    \n",
    "    Args:\n",
    "        generator: The GAN generator model.\n",
    "        dataloader: DataLoader for real images (needed for FID reference).\n",
    "        device: 'cuda' or 'cpu'.\n",
    "        num_imgs: Number of images to generate/use for calculation.\n",
    "                  (Standard for papers is 50k, but 10k is faster for debugging).\n",
    "    \n",
    "    Returns:\n",
    "        fid_score (float), is_score (float)\n",
    "    \"\"\"\n",
    "    fid = FrechetInceptionDistance(feature=2048, normalize=True).to(device)\n",
    "    inception = InceptionScore(normalize=True).to(device)\n",
    "    \n",
    "    generator.eval()\n",
    "    \n",
    "    print(f\"--- Computing Metrics (Samples: {num_imgs}) ---\")\n",
    "    \n",
    "    real_count = 0\n",
    "    for batch in tqdm(dataloader, desc=\"Processing Real Images\"):\n",
    "        batch = batch.to(device)\n",
    "        remaining = num_imgs - real_count\n",
    "        if remaining <= 0:\n",
    "            break\n",
    "            \n",
    "        if batch.shape[0] > remaining:\n",
    "            batch = batch[:remaining]\n",
    "        if batch.min() < 0:\n",
    "            batch = (batch + 1) / 2  # Now [0, 1]\n",
    "            \n",
    "        fid.update((batch * 255).to(torch.uint8), real=True)\n",
    "        real_count += batch.shape[0]\n",
    "\n",
    "    fake_count = 0\n",
    "    while fake_count < num_imgs:\n",
    "        batch_size = min(dataloader.batch_size, num_imgs - fake_count)\n",
    "        \n",
    "        z = torch.randn(batch_size, 100).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            fake_imgs = generator(z)\n",
    "        fake_imgs = (fake_imgs + 1) / 2\n",
    "        fake_uint8 = (fake_imgs * 255).to(torch.uint8)\n",
    "        \n",
    "        fid.update(fake_uint8, real=False)\n",
    "        inception.update(fake_uint8)\n",
    "        \n",
    "        fake_count += batch_size\n",
    "        \n",
    "    print(\"Finalizing calculations...\")\n",
    "    fid_score = fid.compute().item()\n",
    "    is_score_mean, is_score_std = inception.compute()\n",
    "    \n",
    "    return fid_score, is_score_mean.item()\n",
    "print(\"Evaluating WGAN...\")\n",
    "fid_wgan, is_score_wgan = get_evaluation_metrics(\n",
    "    model.generator, \n",
    "    train_loader, \n",
    "    DEVICE, \n",
    "    num_imgs=2000\n",
    ")\n",
    "print(f\"FID: {fid_wgan:.4f} | IS: {is_score_wgan:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aebd0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "discovr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
